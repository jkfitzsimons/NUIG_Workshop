{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentially Private SKLearn with DiffPrivLib\n",
    "\n",
    "Welcome to the lab ðŸ‘‹\n",
    "\n",
    "In this tutorial, we'll learn about differentially private (DP) machine learning using the DiffPrivLib from IBM Research. A cool thing for you to know, is that this library was created by a team just down the road (in the IBM Research Ireland Campus) and is a great example of how differential privacy can be used in the context of machine learning.\n",
    "\n",
    "Before digging in, we'll introduce some concepts to get a sense of what their code is doing. For full details of the DiffPrivLib framework, check out the codebase [here](https://github.com/IBM/differential-privacy-library).\n",
    "\n",
    "ðŸ§ ðŸ¥… : If you aren't interested in SKLearn type models and want to move to look at neural nets or something \"shiny\" like that, why not take a look at [TF Privacy](https://github.com/tensorflow/privacy) and [Opacus](https://github.com/pytorch/opacus) to see how DP can be used to train (or more realistically finnetune) the weights of DNNs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does it mean for a machine learning model to be DP?\n",
    "\n",
    "Good question, glad you asked!\n",
    "\n",
    "A a differentially private machine learning model is one in which the impact of the training od the learned weights/parameters is differentially private. So intuitively you should ask yourself \"if I were to give this model to someone, or it's predictions, could they possibly learn about any individual training sample?\"\n",
    "\n",
    "If you were to think about previous examples from the lecture, the model itself is now the *output* of the differentially private \"query\" performed on the training data ðŸ¤¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the machine learning model know how much noise to apply?\n",
    "\n",
    "Your questions are on ðŸ”¥ today!\n",
    "\n",
    "In general, it depends. If you were to go through the research, what you'll find is \"catch-all\" approaches and \"handcrafted\" approaches.\n",
    "\n",
    "#### Catch-alls via Alternate Optimization Functions\n",
    "\n",
    "What I mean by a catch-all is simply that a large chunk of ML algorithms use an optimization step in order to update the weights/parameters of a model based on training data. So if you can bottlekneck this step to be differentially private (with some usually very small epsilon per step) then your model will be differentially private with some multiple of epsilon times the number of optimisation steps and weights updated. This might seem complex to calculate, but if you know the models DAG of operations, it can usually be calculated auto-magically.\n",
    "\n",
    "This is usually the approach taken with neural networks.\n",
    "\n",
    "#### Handcrafted approaches\n",
    "\n",
    "There are also a lot of effort to find efficient use of privacy loss in order to train models that have a specific structure. For some such models there may lie a trick that lets you get more bang for you buck (so to speak) by reformulating the problem such that the privacy loss during training is deduced. This is also the case for models which do not apply a common step (such as gradient decent) during training and hence require a bit of IQ to be thrown at finding the next best alternative.\n",
    "\n",
    "#### Clipping and Normalizing\n",
    "\n",
    "The above explains how DP is actually applied. But as we heard earlier, knowing the magnitude of noise to apply is also a challenge. For some problems it is very natural that the input data is bounded, like in image prcessing pixels are often a tripple of values ranged between 0 and 255. This is the ideal scenario.\n",
    "\n",
    "If inputs were continuous in plus/minus infinity... we'd have a problem. Typically to avoid this data is \"clipped\". That is to say we either truncate individual values to lie within a fixed domain (like anything over the number 10 becomes 10 for example) or rewieght the norm of the input is rewieghted to lie within a specific bound - that's a little bit like making the inputs as \"realtive\" inputs rather than hard values.\n",
    "\n",
    "![](https://www.tutorialexample.com/wp-content/uploads/2019/11/vector-normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what clipping looks like in practice:\n",
    "\n",
    "#### Norm data exeding limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "clip_limit = 10\n",
    "\n",
    "input_data = [\n",
    "    [1,3,6,9],\n",
    "    [1,2,1,2],\n",
    "    [10,0,100,1000]\n",
    "]\n",
    "\n",
    "norms = np.linalg.norm(input_data, axis=1) / clip_limit\n",
    "norms[norms < 1] = 1\n",
    "\n",
    "output = input_data / norms[:, np.newaxis]\n",
    "\n",
    "print(\"Rows renormalized to be under \"+str(clip_limit)+\":\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clip data exceding limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# purely examplary\n",
    "clip_limit_upper = np.array([4,5,6,7])\n",
    "clip_limit_lower = np.array([-1,0,1,2])\n",
    "\n",
    "input_data = np.array([\n",
    "    [1,3,6,9],\n",
    "    [1,2,1,2],\n",
    "    [10,0,100,1000]\n",
    "])\n",
    "\n",
    "output = np.clip(input_data, clip_limit_lower, clip_limit_upper)\n",
    "\n",
    "print(\"Rows clipped to be between \"+str(clip_limit_upper)+\" and \" +str(clip_limit_lower)+ \":\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think you get the idea at this stage...\n",
    "\n",
    "As we turn our heads to DiffPrivLib you can actually see how this normalization is performed in practice by checking our the source code responsible for these opperations [here](https://github.com/IBM/differential-privacy-library/blob/main/diffprivlib/validation.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the typical trust model of DP Libraries?\n",
    "\n",
    "This is probably the most important question TBH. Adding some noise to data and functions for the sake of it is a bit of a waste of time if it is not _actually_ protecting our data. The trust model of a crypto library is basically scenario in which it is assumed to be applied.\n",
    "\n",
    "In most cases of DP machine learning / statistics frameworks (OpenDP, DiffPrivLib, etc) the setting is assumed to be a *trusted curator model*.\n",
    "\n",
    "#### Trusted Curator Models\n",
    "\n",
    "A trusted curator model basically means that the person who is applying the differential privacy is allowed to now about the sensitive data. However, they are doing this for the purpose of output disclosure control. Essentially, you trust the person who is doing the model fitting but not thos who the model is given to upstream.\n",
    "\n",
    "#### Malicious Querier\n",
    "\n",
    "This is a much stronger claim, I believe PySyft by OpenMined is aiming for this, but they have some way to go to get it ready for production yet. Essentially in this scenario you don't trust the querier who will try to exploit anything and everything they can to learn more than they are meant to. In such a scenario, typically the queriers opperations need to be tightly constained and validated. You also have to be concerned with leaking side information like how long the query took, the resolution of the noise applied, etc. In such a setting you almost certainly want to have external security reviews and pentesting prior to moving to production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiffPrivLib: Time to do some Noisy Learning!\n",
    "\n",
    "OK, so from the lecture you now have some idea of how differential privacy works, and you get the gist of how it can be applied in the context of statistics and machine learning. But you want to see it actually work in practice!\n",
    "\n",
    "There are of course many frameworks you can use, but what better way that to use a toolbox created by fellow Irish-based reseachers designed to provived a similar interface to SKLearn! Meet *DiffPrivLib*.\n",
    "\n",
    "First off go ahead and install the library with pip install:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install diffprivlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it's installed, let's go ahead and import it into the notebook along with numpy and the real sklear (just for comparison)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import diffprivlib.models as dp\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a dataset, UCI has loads of course of you could mix it up with something from kaggle or a open dataset - totally your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.loadtxt(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "                        usecols=(0, 4, 10, 11, 12), delimiter=\", \")\n",
    "\n",
    "y_train = np.loadtxt(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "                        usecols=14, dtype=str, delimiter=\", \")\n",
    "\n",
    "X_test = np.loadtxt(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\",\n",
    "                        usecols=(0, 4, 10, 11, 12), delimiter=\", \", skiprows=1)\n",
    "\n",
    "y_test = np.loadtxt(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\",\n",
    "                        usecols=14, dtype=str, delimiter=\", \", skiprows=1)\n",
    "# Must trim trailing period \".\" from label\n",
    "y_test = np.array([a[:-1] for a in y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically the task here is to predict if someone earns more or less than 50k per year (it's a really old data set that's well known to be quite easy to do ok with). The inputs are:\n",
    "\n",
    "- age: continuous. <- in range [0,100]\n",
    "- education-num: continuous. <- in range [0, 16] \n",
    "- capital-gain: continuous.\n",
    "- capital-loss: continuous.\n",
    "- hours-per-week: continuous. <- easily capped at 12*7, thus in range [0,84]\n",
    "\n",
    "and the output is a binary class '<=50K' or '>50K'.\n",
    "\n",
    "#### Go ahead and explore the data to get a sense for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write any data exploration code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK let's create a benchmark of the non-DP logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver=\"lbfgs\")\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with no noise applied, the accuracy of a logistic regression model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = clf.score(X_test, y_test)\n",
    "print(\"Non-private test accuracy: %.2f%%\" % (baseline * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to differential privacy this thing! ðŸ’ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_clf = dp.LogisticRegression()\n",
    "dp_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woops - bet you just got a privacy leak! \n",
    "\n",
    "That's because the model doesn't know how much noise to apply without actually looking at the data itself. We're better off to specify a data norm so that data with any norm larger that that will be re normalised to fit inside the bounds (like we discussed earlier).\n",
    "\n",
    "What do you think is a reasonable bound to apply?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your data norm you think is reasonable considering you understand the data features\n",
    "data_norm = 1234 # just an example\n",
    "\n",
    "# we'll now add it as an input \n",
    "dp_clf = dp.LogisticRegression(data_norm=data_norm)\n",
    "dp_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Differentially private test accuracy (epsilon=%.2f): %.2f%%\" % \n",
    "     (dp_clf.epsilon, dp_clf.score(X_test, y_test) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, what would happen if we were to set the epsilon to a really big number or rather to infinity!? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_clf = dp.LogisticRegression(epsilon=float(\"inf\"), data_norm=1e5)\n",
    "dp_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¥ Nailed it! We are back to the same situation as if there was no differential privacy applied at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Agreement between non-private and differentially private (epsilon=inf) classifiers: %.2f%%\" % \n",
    "     (dp_clf.score(X_test, clf.predict(X_test)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK last step of the walk through, let's have a look at how the epsilon decrease (ie the increase in privacy) effects the accuracy of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "epsilons = np.logspace(-3, 1, 500)\n",
    "\n",
    "for eps in epsilons:\n",
    "    dp_clf = dp.LogisticRegression(epsilon=eps, data_norm=100)\n",
    "    dp_clf.fit(X_train, y_train)\n",
    "    accuracy.append(dp_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "plt.semilogx(epsilons, accuracy, label=\"Differentially private\")\n",
    "plt.plot(epsilons, np.ones_like(epsilons) * baseline, dashes=[2,2], label=\"Non-private\")\n",
    "plt.title(\"Differentially private logistic regression accuracy\")\n",
    "plt.xlabel(\"epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(epsilons[0], epsilons[-1])\n",
    "plt.legend(loc=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over to you!!\n",
    "\n",
    "OK you're a high flying NUIG grad student - I think you got this next task!\n",
    "\n",
    "The goal is to use the same inputs and outputs but mix up the models you are using. A list of the DiffPrivLib models are here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in dir(dp):\n",
    "    if model_name[0].isupper():\n",
    "        print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source are here if you need it: https://github.com/IBM/differential-privacy-library/tree/main/diffprivlib/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try out alternative DP models below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
